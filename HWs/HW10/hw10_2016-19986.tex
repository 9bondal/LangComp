\documentclass[a4paper,11pt]{scrartcl}
\usepackage[margin=1in]{geometry} % decreases margins
\usepackage{setspace}
\setlength{\parskip}{0pt}
\onehalfspacing

\usepackage{enumitem}
\setlist{noitemsep, topsep=0pt, partopsep=0pt, leftmargin=*}

\usepackage{textcomp}
\usepackage{booktabs}
\usepackage[mathrm=sym]{unicode-math}

\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
  colorlinks=true,% false: boxed links; true: colored links
  linkcolor=blue,        % color of internal links
  citecolor=blue,   % color of links to bibliography
  filecolor=magenta,     % color of file links
  urlcolor=blue
}
\urlstyle{rm}

\usepackage[procnames]{listings}
\usepackage[dvipsnames]{xcolor}

\definecolor{stringcolor}{RGB}{233,125,44}

\makeatletter
\lstdefinestyle{mystyle}{
  language=python,
  showstringspaces=false,
  otherkeywords={__eq__,__setitem__,__abs__},
  basicstyle=%
    \ttfamily
    \lst@ifdisplaystyle\normalsize\fi,
    numberstyle=\ttfamily\small\color{gray},
    keywordstyle=\color{Blue},
    commentstyle=\color[gray]{0.6},
    stringstyle=\color{stringcolor},
    procnamekeys={class},
    procnamestyle=\color{Bittersweet},
    emph={None,True,False},
    emphstyle=\itshape\color[rgb]{0.7,0,0},
    emph={[2]self},
    emphstyle=[2]\itshape\color{Bittersweet},
    % numbers=left,
    texcl=true
}
\makeatother

\lstset{style=mystyle}
\lstset{escapeinside={(*@}{@*)}}

\usepackage{fontspec}
\usepackage{bold-extra}
\setmonofont[AutoFakeSlant=0.2,Scale=0.95]{D2Coding}
\setsansfont[BoldFont=AppleSDGothicNeo-SemiBold]{Apple SD Gothic Neo}
\setmainfont[AutoFakeSlant=0.2,BoldFont=SDMyeongjoNeoa-eSm,WordSpace={1.0,0.5,0.5},Kerning=On]{SDMyeongjoNeoa-bLt}

\usepackage{longfbox}
\usepackage{kotex}

\addtokomafont{labelinglabel}{\bfseries}
\addtokomafont{title}{\bfseries}

\setkomafont{disposition}{\normalfont}
\setkomafont{section}{\LARGE\bfseries\sffamily}
\setkomafont{subsection}{\Large\bfseries\sffamily}

\usepackage{indentfirst}
\usepackage{graphicx}

\newcommand{\kostr}[1]{\textcolor{stringcolor}{#1}}

\title{\vspace{-0.5in}LangComp HW10}
\author{\vspace{-15pt}2016-19986 정누리}
\date{\vspace{-5pt}\today}

%++++++++++++++++++++++++++++++++++++++++

\begin{document}

\maketitle

\setcounter{section}{1}
\setcounter{subsection}{-1}
\subsection{사전 준비}
\begin{lstlisting}
  import numpy as np

  N = 1410000000

  occur = {
        1: {'(*@\kostr{하늘은}@*)': 3520000, '(*@\kostr{파랗고}@*)': 392000, '(*@\kostr{단풍잎은}@*)': 34600,
            '(*@\kostr{빨갛고}@*)': 339000, '(*@\kostr{은행잎은}@*)': 24300, '(*@\kostr{노랗고}@*)': 359000},
        2: {'(*@\kostr{하늘은 파랗고}@*)': 56100, '(*@\kostr{파랗고 단풍잎은}@*)': 23,
            '(*@\kostr{단풍잎은 빨갛고}@*)': 160, '(*@\kostr{빨갛고 은행잎은}@*)': 85,
            '(*@\kostr{은행잎은 노랗고}@*)': 198},
        3: {'(*@\kostr{하늘은 파랗고 단풍잎은}@*)': 34, '(*@\kostr{파랗고 단풍잎은 빨갛고}@*)': 0,
            '(*@\kostr{단풍잎은 빨갛고 은행잎은}@*)': 3, '(*@\kostr{빨갛고 은행잎은 노랗고}@*)': 85}
  }

  test = '(*@\kostr{하늘은 파랗고 단풍잎은 빨갛고 은행잎은 노랗고}@*)'
\end{lstlisting}

\subsection{모형별 확률 계산}
\begin{lstlisting}
  def smooth(corpus, k=1):
      result = dict()
      for n in corpus:
          logN = np.log(N + k * len(corpus[n]))
          result[n] = {key: np.log(val + k) - logN
                      for key, val in corpus[n].items()}
      return result

  def prob_ngram(n, sent, corpus):
      words = sent.split()
      if not words:
          return 0.


      if n == 1:
          logp = np.sum([corpus[n][w] for w in words])
      else:
          logp = corpus[1][words[0]]

          for end in range(2, n):
              k_nm1 = ' '.join(words[:end-1])
              k_n   = ' '.join((k_nm1, words[end-1]))
              logp += corpus[end][k_n] - corpus[end-1][k_nm1]

          for end in range(n, len(words)):
              k_nm1 = ' '.join(words[end-n:end-1])
              k_n   = ' '.join((k_nm1, words[end-1]))
              logp += corpus[n][k_n] - corpus[n-1][k_nm1]

      return np.exp(logp)

  if __name__ == '__main__':
      # By smoothing
      smoothed = smooth(occur)
      p_all = [(n, prob_ngram(n, test, smoothed))
               for n in smoothed]

      print('Smoothed probability: ')
      for res in p_all:
          print(*res)
\end{lstlisting}

\lstinline{smooth} 함수는 smoothing을 실행하여 모든 occurence에 \lstinline{k}만큼을 더한 이후, 각 N-그램마다의 확률의 로그값을 가지는 \lstinline{dict}를 돌려주는 함수이다. \lstinline{prob_ngram}은 평탄화된 corpus를 이용하여 실제 N-그램 확률을 계산하는 함수이다.

실제 계산 결과는 아래와 같다.

\par\medskip
\begin{longfbox}[margin-left=1em]
  \ttfamily
  \noindent Smoothed probability: \\
  1 1.7969033083061857e-23 \\
  2 2.8754793536134484e-15 \\
  3 2.5696371635870976e-11
\end{longfbox}

\subsection{모형별 복잡도 평가}
\begin{lstlisting}
  def perplx(p, k):
      return np.power(p, -1/k)


  if __name__ == '__main__':
      # Perplexity
      split = test.split()
      p_all_np = np.asarray([p for _, p in p_all])
      pplx = perplx(p_all_np, len(split))

      print("\nPerplexity: ")
      for n, res in enumerate(pplx, 1):
          print(n, res)
\end{lstlisting}

간단히 위와 같이 계산할 수 있다. 이때, \lstinline{p_all}은 앞서 계산한 결과와 동일하며, 위 코드의 실행 결과는 아래와 같다.

\par\medskip
\begin{longfbox}[margin-left=1em]
  \ttfamily
  \noindent Perplexity: \\
  1 6178.919662636821 \\
  2 265.18459466995836 \\
  3 58.21318560783996
\end{longfbox}

\subsection{보간법}

\begin{lstlisting}
  def recursive_get(n, start, words, corpus, default=0.):
      if n <= 0:
          # Recursion guard
          return default

      key = ' '.join(words[start:start+n])
      return corpus[n][key] if key in corpus[n] else \
          recursive_get(n-1, start+1, words, corpus, default=default)

  def interploate(sent:str, lbds:tuple, corpus:dict):
      words = sent.split()
      if not words:
          return 0.

      calc_mat = [
          [recursive_get(n, end-n, words, corpus) /
           recursive_get(n-1, end-n, words, corpus, default=1)
           for n in range(1, len(lbds) + 1)]
          for end in range(1, len(words) + 1)
      ]
      interpolated_probs = np.dot(np.asarray(calc_mat),
                                  np.flip(np.asarray(lbds)))
      return np.exp(np.sum(np.log(interpolated_probs)))

  if __name__ == '__main__':
      # By interpolation
      no_smooth = {n: {k: v / N for k, v in corpus.items()}
                  for n, corpus in occur.items()}

      print("\nInterpolated probability: ")
      print(interploate(test, (0.5, 0.3, 0.2), no_smooth))
      print(interploate(test, (0.7, 0.2, 0.1), no_smooth))
\end{lstlisting}

다소 복잡한데, \lstinline{recursive_get} 함수는 문제의 조건에서 N-그램의 문장 시작 시의 조건부 확률을 (N-1)-그램으로 계속 delegation하는 방식을 실제로 구현하기 위해 도입하였다. 또한 \lstinline{corpus}는 여기에서는 로그 확률이 아닌 단순 확률을 값으로 가지며, \lstinline{calc_mat}은 각 단어의 등장 조건부 확률만을 모두 계산한 \lstinline{(len(words), len(lbds))} 크기의 list of lists이다.

이후 해당 matrix를 \lstinline{numpy.ndarray}로 변환한 뒤 계수인 \lstinline{lbds}와의 dot product를 계산하여 interpolated된 확률을 각 단어별로 계산한 뒤, 마지막으로 그 결과를 로그를 취하여 더하고 다시 exponential을 취하여 계산하였다.

실제 계산 결과는 아래와 같다.

\par\medskip
\begin{longfbox}[margin-left=1em]
  \ttfamily
  \noindent Interpolated probability: \\
  7.096168276562857e-14 \\
  1.3800156119401259e-13
\end{longfbox}

\end{document}
